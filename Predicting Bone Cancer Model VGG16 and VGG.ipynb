{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Bone Cancer Model VGG16 and VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# import the libraries as shown below\n",
    "# we are using VGG16 and VGG19\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "print(tf.__version__) #here check the tf version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard model name with time function\n",
    "#make a folder named logs\n",
    "CANCER = \"predict-bone-cancer-cnn-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir = 'logs/{}'.format(CANCER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize all train images\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "# we are train our model with 84 images\n",
    "# we are providing here two folder for training \n",
    "\n",
    "train_path = 'Datasets/train'\n",
    "valid_path = 'Datasets/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
    "# Here we are providing the \"IMAGE_SIZE\" and also the RGB dimenssion 3\n",
    "# Here we will be using imagenet weights for categorises the images\n",
    "# Basically VGG16 used for classify thousand different categorises  of images that's why \"include_top = False\". \n",
    "# Because we are training 2 categorises images. So we are droping the last column\n",
    "# Also droping the first column because we are providing the \"IMAGE_SIZE\"\n",
    "\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "# Putting the for loop and make sure that all the layer shouldn't be train\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many classes or categorises in my train dataset\n",
    "folders = glob('Datasets/train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the droping the first & last layer\n",
    "# We just the the Flatten layer\n",
    "# Flatten layer basically converts the features map to a single column\n",
    "\n",
    "x = Flatten()(vgg.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally adding the last layer(Dense) and also provide the folder length. We need how many categories we have.\n",
    "# Then we are using the activation function called \"softmax\"\n",
    "\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "\n",
    "# Then combine the vgg output and input this will create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 50178     \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# view  the model strcture \n",
    "# Absorve the last layer(Dense) it is just having 2 output categorises\n",
    "# Because our dataset  have 2 categorises \n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fit & A File Create For Loading the model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "# Adam optimizer provide an optimization algorithm which can handle sparse gradients on noisy problems.\n",
    "\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Image Data Generator to import the images from the dataset\n",
    "# we upload dataset using ImageDataGenerator which will help in train and test dataset.\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# we insert images using flow_from_directory . For batch size, at a time 32 images will demand for training\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('Datasets/train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('Datasets/test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-0b51bedac158>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/8\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6811 - accuracy: 0.5385WARNING:tensorflow:From C:\\Users\\hasib\\.conda\\envs\\thesis21\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/2 [==============================] - 20s 10s/step - loss: 1.2976 - accuracy: 0.6889 - val_loss: 1.9194 - val_accuracy: 0.7436\n",
      "Epoch 2/8\n",
      "2/2 [==============================] - 15s 8s/step - loss: 1.2620 - accuracy: 0.7778 - val_loss: 0.8712 - val_accuracy: 0.7436\n",
      "Epoch 3/8\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.4124 - accuracy: 0.7556 - val_loss: 1.1163 - val_accuracy: 0.4615\n",
      "Epoch 4/8\n",
      "2/2 [==============================] - 15s 8s/step - loss: 0.5536 - accuracy: 0.6889 - val_loss: 0.8038 - val_accuracy: 0.7692\n",
      "Epoch 5/8\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.2200 - accuracy: 0.8444 - val_loss: 1.4340 - val_accuracy: 0.7436\n",
      "Epoch 6/8\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.4631 - accuracy: 0.7778 - val_loss: 1.6217 - val_accuracy: 0.7436\n",
      "Epoch 7/8\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.3052 - accuracy: 0.8667 - val_loss: 1.2004 - val_accuracy: 0.7949\n",
      "Epoch 8/8\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.1030 - accuracy: 0.9556 - val_loss: 0.8757 - val_accuracy: 0.7949\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# we use fit_generator from library for running it. we will use train data set for running and test data for validation.\n",
    "#tensorboard callbacks function for visualize accuracy and loss graph\n",
    "\n",
    "r = model.fit_generator(\n",
    "    training_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=8,\n",
    "    steps_per_epoch=len(training_set),\n",
    "    validation_steps=len(test_set),\n",
    "    callbacks = [tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a h5 file\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('our-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting based on the model\n",
    "        \n",
    "    1. Import all the necessary libraries we need available in keras\n",
    "    6. Load the model vgg19 to categorize thousand of different categories.\n",
    "    7. load image set we have use for the project,we load training path. Here we set target size as 224,224 because here        we are working in RGB type of images as it is 3D.\n",
    "    8. Convert to numpy array\n",
    "    9. Expand the shape of the array. A new axis will appear at the axis position in the expanded array shape.\n",
    "    10. Preprocessing input x\n",
    "    11. It returns the probability for each outcome class as a value between 0 and 1.\n",
    "    12. set classes for malignant\n",
    "    13. set classes for normal\n",
    "    15. Set a condition if malignant print malignant. Otherwise print normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: malignant\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "import numpy as np\n",
    "model=load_model('our-model.h5')\n",
    "img=image.load_img('Datasets/val/malignant/m (5).JPG', target_size=(224, 224))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "img_data=preprocess_input(x)\n",
    "classes=model.predict(img_data)\n",
    "malignant=classes[0,0]\n",
    "normal=classes[0,1]\n",
    "\n",
    "if(malignant==1):\n",
    "    print('P: malignant')\n",
    "else:\n",
    "    print('P: normal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
